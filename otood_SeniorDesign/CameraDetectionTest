import pyrealsense2 as rs
import numpy as np
import cv2

# RealSense initialization
# Set up the RealSense pipeline to capture depth and color streams
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)  # Enable depth stream at 640x480 resolution and 30 FPS
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)  # Enable color stream at 640x480 resolution and 30 FPS
pipeline.start(config)  # Start the RealSense pipeline

# Align depth to color
# Align depth data to match the perspective of the color camera
align_to = rs.stream.color
align = rs.align(align_to)

try:
    while True:
        # Capture frames
        # Wait for a set of frames (depth and color)
        frames = pipeline.wait_for_frames()
        aligned_frames = align.process(frames)  # Align depth frame to the color frame

        depth_frame = aligned_frames.get_depth_frame()  # Extract the depth frame
        color_frame = aligned_frames.get_color_frame()  # Extract the color frame

        if not depth_frame or not color_frame:
            continue  # Skip if frames are not available

        # Convert frames to numpy arrays
        depth_image = np.asanyarray(depth_frame.get_data())  # Convert depth frame to numpy array
        color_image = np.asanyarray(color_frame.get_data())  # Convert color frame to numpy array

        # Normalize depth image to 8-bit for mask creation
        depth_normalized = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)

        # Apply GaussianBlur to reduce noise in depth image
        depth_blurred = cv2.GaussianBlur(depth_normalized, (5, 5), 0)  # Smooth depth image to reduce noise

        # Create near and far masks
        # Generate a mask for near objects (items on the desk)
        near_mask = cv2.inRange(depth_blurred, 120, 200)  # Pixel values in range 120-200 represent near objects
        # Generate a mask for far objects (background)
        far_mask = cv2.inRange(depth_blurred, 0, 119)  # Pixel values below 120 represent the background

        # Combine masks to create GrabCut initialization mask
        mask = np.full_like(near_mask, cv2.GC_BGD, dtype=np.uint8)  # Default to background
        mask[far_mask == 255] = cv2.GC_PR_BGD  # Mark far regions as "probably background"
        mask[near_mask == 255] = cv2.GC_FGD  # Mark near regions as "foreground"

        # Morphological operations to refine masks
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Define a rectangular kernel
        near_mask = cv2.morphologyEx(near_mask, cv2.MORPH_CLOSE, kernel, iterations=2)  # Refine near mask

        # Apply GrabCut
        bg_model = np.zeros((1, 65), np.float64)  # Background model for GrabCut
        fg_model = np.zeros((1, 65), np.float64)  # Foreground model for GrabCut
        cv2.grabCut(color_image, mask, None, bg_model, fg_model, 5, cv2.GC_INIT_WITH_MASK)  # Run GrabCut

        # Extract the segmented foreground
        segmented = np.where((mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD), 255, 0).astype('uint8')  # Create a binary mask for the foreground
        foreground = cv2.bitwise_and(color_image, color_image, mask=segmented)  # Apply mask to color image

        # Display the results
        cv2.imshow('Original Color Frame', color_image)  # Show the original color frame
        cv2.imshow('Segmented Foreground', foreground)  # Show the segmented foreground

        # Exit on 'q' key
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break  # Exit the loop
finally:
    pipeline.stop()  # Stop the RealSense pipeline
    cv2.destroyAllWindows()  # Close all OpenCV windows
